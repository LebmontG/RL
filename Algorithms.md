# 有模型

## 动态规划

### 值

### 策略

## NAF

## 异步动态规划

# 无模型

## 值

### Q-Learning

对于每种状态，利用Q函数计算其值，根据最大值选择动作。Q函数的参数根据之前每一步交互的奖励迭代生成，重复的计算由从前向后的顺序省去，因此是动态规划方法。
Q函数可以输入状态-动作对输出值，也可以输入状态，输出每种动作的值，后一种Q函数难以处理连续动作。
Q函数可以是表格，也可以是网络。表格难以处理连续状态的情况。

### sarsa（state，action，reward，state，action）

### DDPG（深度确定性策略梯度）

与Q-Learning相比，加入了策略网络以处理连续动作，Q函数的输入是状态-动作对。实际上，这种算法并非基于策略，而是基于值，因为输出是由最大值确定的动作而不是概率向量。

## 策略

### PPO（近端策略优化）

与基本的策略迭代相比，构造一个网络用于交互，一个用于更新。因为如果交互时，更新的网络即最后的网络，则网络每一步都会变化，每一步的数据只能用一次，然而强化学习的数据是相当宝贵的，所以对用与交互的网络的数据进行数学处理，就可以重复使用，即将在线策略改变为离线策略。

### AC

AC结构是同时具有Q函数和策略函数的结构。
与基本的策略迭代相比，加入了Q网络，每一步都要更新两个网络。

### TRPO
